---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

# Set up

```{python}
import os
import datetime
import pandas as pd
import matplotlib.pyplot as plt
import altair as alt
import geopandas as gpd
import shapely
import numpy as np
from shapely import Polygon, Point
from numpy import mean, nan # PS: Some version of numpy only consider NaN. So graders should consider this when this chunk of code is ran.
```

## Download and explore the Provider of Services (POS) file (10 pts)

1. The variable that we pulled for are :
    - PRVDR_CTGRY_SBTYP_CD
    - PRVDR_CTGRY_CD
    - PRVDR_NUM
    - PGM_TRMNTN_CD
    - FAC_NAME
    - ZIP_CD

2. We will import the POS data for Q4 of year 2016 and filter them to have only the facilities with provider type code 01 and subtype code 01.

```{python}
# Importing the data for POS in Q4 of 2016

pos_files_path = "N:/3 MES DOSSIERS SECONDAIRES/MASTER PREPARATION PROGRAM/University of Chicago/DAP II/problem-set-4-lara-ralph"

# First, we will convert the PRVDR_CTGRY_SBTYP_CD, PRVDR_CTGRY_CD and PGM_TRMNTN_CD values into strings are they represent categories (nominal data types) instead of quantitative, as described in the data dictionnary. We will be cautious also to convert from float to int type before. 
# Then we will filter to only have the facilities with provider type code 01 and subtype code 01.

# As we might use this process for 2017Q4, 2018Q4, and 2019Q4, let's create two functions for those processes instead. This will save us time and use less code lines

def read_and_convert_tostr(file_name_instring, absolute_path):
    # Reading the file
    file_name_extension = file_name_instring + '.csv'
    relative_path = os.path.join(absolute_path, file_name_extension)
    dataset = pd.read_csv(relative_path, encoding='latin1')

    # Converting the PRVDR_CTGRY_SBTYP_CD values into strings
    category_to_str = []
    for value in dataset['PRVDR_CTGRY_SBTYP_CD']:
        if np.isnan(value) == True:
            category_to_str.append(value)
        elif np.isnan(value) == False:
            category_to_str.append(str(int(value)))
    dataset['PRVDR_CTGRY_SBTYP_CD'] = category_to_str

    # Converting the PRVDR_CTGRY_CD values into strings
    subcategory_to_str = []
    for value in dataset['PRVDR_CTGRY_CD']:
        if np.isnan(value) == True:
            subcategory_to_str.append(value)
        elif np.isnan(value) == False:
            subcategory_to_str.append(str(int(value)))
    dataset['PRVDR_CTGRY_CD'] = subcategory_to_str

    # Converting the PGM_TRMNTN_CD values into strings
    termination_to_str = []
    for value in dataset['PGM_TRMNTN_CD']:
        if np.isnan(value) == True:
            termination_to_str.append(value)
        elif np.isnan(value) == False:
            termination_to_str.append(str(int(value)))
    dataset['PGM_TRMNTN_CD'] = termination_to_str

    # Filtering to only have the facilities with provider type code 01 and subtype code 01.
    dataset_filtered = dataset[(dataset['PRVDR_CTGRY_SBTYP_CD'] == '1') & (dataset['PRVDR_CTGRY_CD'] == '1')]
    return dataset_filtered

# Using the function to create the filtered dataset

pos2016_filtered = read_and_convert_tostr('pos2016', pos_files_path)
```

    a. To find the number of hospitals reported in this data, we are considering two options. First, we are considering the number of observations in the datasets. But second, we are also considering the unique values for the FAC_NAME variable.

```{python}
print('If we consider the number of observations, there are', len(pos2016_filtered), 'hospitals reported in the data.\n')
print('However, if we consider only the unique facility names, there are', len(pos2016_filtered['FAC_NAME'].unique()), 'hospitals reported in this data.') # There was no nan in this column
```
This number doesn't seem to make sense considering the approximation reported in the article from the Kaiser Family Foundation which states that "There are nearly 5,000 short-term, acute care hospitals in the United States.". The number we found is way larger than the 5,000 mentioned.

    b. This number doesn't seem to match also the date published by the American Hospital Association, which stated that there were 5,534 hospitals in the US in 2016, from which 88% were Community (Non-Federal Acute Care).
    # https://www.aha.org/system/files/2018-01/Fast%20Facts%202018%20pie%20charts.pdf

3. Let's now repeat the previous process for 2017Q4, 2018Q4, and 2019Q4, to later append all of the datasets.

```{python}
# Using the function we previously created, we are creating three more filtered datasets: for 2017Q4, 2018Q4, and 2019Q4.
pos2017_filtered = read_and_convert_tostr('pos2017', pos_files_path)
pos2018_filtered = read_and_convert_tostr('pos2018', pos_files_path)
pos2019_filtered = read_and_convert_tostr('pos2019', pos_files_path)

# Before appending the four datasets, to make sure we can identify which data is related to each specific year, we will create a new column in each of those dataframes to set the year.
pos2016_filtered['YEAR'] = 2016
pos2017_filtered['YEAR'] = 2017
pos2018_filtered['YEAR'] = 2018
pos2019_filtered['YEAR'] = 2019

# Now, we can append the datasets.
pos_16to19_combined = pd.concat([pos2016_filtered, pos2017_filtered, pos2018_filtered, pos2019_filtered])
```

Now we can plot the number of observations in the combined dataset by year.

```{python}
# Grouping by year
pos_group_byyear = pos_16to19_combined.groupby('YEAR')
num_obs_peryear = pos_group_byyear.apply(lambda group: len(group))
num_obs_peryear = num_obs_peryear.reset_index()
num_obs_peryear.columns = ['YEAR', 'NUM_OBSERVATIONS']

# Creating the plot now
graph_num_obs_peryear = alt.Chart(num_obs_peryear).mark_line().encode(
    alt.X('YEAR:N', title = 'Year'),
    alt.Y('NUM_OBSERVATIONS:Q',  title = 'Number of observations',
    scale=alt.Scale(domain=[7200, 7400]))
).properties(
    title = 'Number of short-term hospitals over 2016-2019',
    width = 500,
    height = 200
)
(graph_num_obs_peryear + graph_num_obs_peryear.mark_point(fill='red')).display()
```

4. 
    a. Now, we will plot the number of unique hospitals the combined dataset per year

```{python}
# We can use the same group we built previously (by year)
num_unique_hospitals = pos_group_byyear.apply(lambda group: len(group['PRVDR_NUM'].unique()))
num_unique_hospitals = num_unique_hospitals.reset_index()
num_unique_hospitals.columns = ['YEAR', 'NUM_UNIQUE_HOSPITAL']

# Creating the new plot
graph_unique_hospital_peryear = alt.Chart(num_unique_hospitals).mark_line(color = 'red').encode(
    alt.X('YEAR:N', title = 'Year'),
    alt.Y('NUM_UNIQUE_HOSPITAL:Q',  title = 'Number of CMS certificaion number',
    scale=alt.Scale(domain=[7200, 7400]))
).properties(
    title = 'Number of unique hospitals over 2016-2019',
    width = 500,
    height = 200
)
(graph_unique_hospital_peryear + graph_unique_hospital_peryear.mark_point(fill='black')).display()
```
    b. This plot is exactly the same as the plot before. This tells two things that there might be facilities with the same name but with different CMS certification number. 

```{python}
# As a matter of fact, in 2016, we can see the 5 facilities with the highest number of different CMS certification number
print(pos2016_filtered.groupby('FAC_NAME').apply(lambda group: len(group['PRVDR_NUM'].unique())).sort_values(ascending = False).head(5))
```
This also tells that some of those facilities could be operating in multiple locations (meaning having different ZIP CODE in the dataset) or could be changing status or ownership from year to year.

## Identify hospital closures in POS file (15 pts) (*)

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
