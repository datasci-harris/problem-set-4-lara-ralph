---
title: "Problem Set 4 - DAP II"
author: "Ralph Valery VALIERE and Lara Tamer"
date: "11/03/2024"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
    eval: true
    echo: true
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Ralph Valery Valiere - ralphvaleryv
    - Partner 2 (name and cnet ID): Lara Tamer - ltamer
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: LT and RVV \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used by Ralph this pset: 1 \*\*\_\_\*\* Late coins left after submission: 3 \*\*\_\_\*\*
   Late coins used by Lara this pset: 1 \*\*\_\_\*\* Late coins left after submission: 2 \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

# Set up
```{python}
import os
import datetime
import time
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import altair as alt
import geopandas as gpd
import shapely
import warnings
import numpy as np
from shapely import Polygon, Point
from numpy import mean, nan # PS: Some version of numpy only consider NaN. So graders should consider this when this chunk of code is ran.
alt.renderers.enable("png")
warnings.filterwarnings('ignore')
```

## Download and explore the Provider of Services (POS) file (10 pts)

1. The variable that we pulled for are :
    - PRVDR_CTGRY_SBTYP_CD
    - PRVDR_CTGRY_CD
    - PRVDR_NUM
    - PGM_TRMNTN_CD
    - FAC_NAME
    - ZIP_CD

2. We will import the POS data for Q4 of year 2016 and filter them to have only the facilities with provider type code 01 and subtype code 01.

```{python}
# Importing the data for POS in Q4 of 2016

pos_files_path = "N:/3 MES DOSSIERS SECONDAIRES/MASTER PREPARATION PROGRAM/University of Chicago/DAP II/problem-set-4-lara-ralph"

# First, we will convert the PRVDR_CTGRY_SBTYP_CD, PRVDR_CTGRY_CD, PGM_TRMNTN_CD and ZIP_CD values into strings are they represent categories (nominal data types) instead of quantitative, as described in the data dictionnary. We will be cautious also to convert from float to int type before. 
# Then we will filter to only have the facilities with provider type code 01 and subtype code 01.

# As we might use this process for 2017Q4, 2018Q4, and 2019Q4, let's create two functions for those processes instead. This will save us time and use less code lines

def read_and_convert_tostr(file_name_instring, absolute_path):
    file_name_extension = file_name_instring + '.csv'
    relative_path = os.path.join(absolute_path, file_name_extension)
    dataset = pd.read_csv(relative_path, encoding = 'latin1')

    # Converting the PRVDR_CTGRY_SBTYP_CD values into strings
    category_to_str = []
    for value in dataset['PRVDR_CTGRY_SBTYP_CD']:
        if np.isnan(value) == True:
            category_to_str.append(value)
        elif np.isnan(value) == False:
            category_to_str.append(str(int(value)))
    dataset['PRVDR_CTGRY_SBTYP_CD'] = category_to_str

    # Converting the PRVDR_CTGRY_CD values into strings
    subcategory_to_str = []
    for value in dataset['PRVDR_CTGRY_CD']:
        if np.isnan(value) == True:
            subcategory_to_str.append(value)
        elif np.isnan(value) == False:
            subcategory_to_str.append(str(int(value)))
    dataset['PRVDR_CTGRY_CD'] = subcategory_to_str

    # Converting the PGM_TRMNTN_CD values into strings
    termination_to_str = []
    for value in dataset['PGM_TRMNTN_CD']:
        if np.isnan(value) == True:
            termination_to_str.append(value)
        elif np.isnan(value) == False:
            termination_to_str.append(str(int(value)))
    dataset['PGM_TRMNTN_CD'] = termination_to_str

    # Converting the ZIP_CD values into strings
    zipcode_to_str = []
    for value in dataset['ZIP_CD']:
        if np.isnan(value) == True:
            zipcode_to_str.append(value)
        elif np.isnan(value) == False:
            zipcode_to_str.append(str(int(value)))
    dataset['ZIP_CD'] = zipcode_to_str

    # Filtering to only have the facilities with provider type code 01 and subtype code 01.
    dataset_filtered = dataset[(dataset['PRVDR_CTGRY_SBTYP_CD'] == '1') & (dataset['PRVDR_CTGRY_CD'] == '1')]
    return dataset_filtered

pos2016_filtered = read_and_convert_tostr('pos2016', pos_files_path)
```

    a. To find the number of hospitals reported in this data, we are considering two options. First, we are considering the number of observations in the datasets. But second, we are also considering the unique values for the FAC_NAME variable.

```{python}
print(f'If we consider the number of observations, there are {len(pos2016_filtered)} hospitals reported in the data.\n')
print(f"However, if we consider only the unique facility names, there are {len(pos2016_filtered['FAC_NAME'].unique())} hospitals reported in this data.")
```

This number doesn't seem to make sense considering the approximation reported in the article from the Kaiser Family Foundation which states that "There are nearly 5,000 short-term, acute care hospitals in the United States.". The number we found is way larger than the 5,000 mentioned.

    b. This number doesn't seem to match also the date published by the American Hospital Association, which stated that there were 5,534 hospitals in the US in 2016, from which 88% were Community (Non-Federal Acute Care).
    # https://www.aha.org/system/files/2018-01/Fast%20Facts%202018%20pie%20charts.pdf

3. Let's now repeat the previous process for 2017Q4, 2018Q4, and 2019Q4, to later append all of the datasets.

```{python}
# Using the function we previously created, we are creating three more filtered datasets: for 2017Q4, 2018Q4, and 2019Q4.
pos2017_filtered = read_and_convert_tostr('pos2017', pos_files_path)
pos2018_filtered = read_and_convert_tostr('pos2018', pos_files_path)
pos2019_filtered = read_and_convert_tostr('pos2019', pos_files_path)

# Before appending the four datasets, to make sure we can identify which data is related to each specific year, we will create a new column in each of those dataframes to set the year.
pos2016_filtered['YEAR'] = 2016
pos2017_filtered['YEAR'] = 2017
pos2018_filtered['YEAR'] = 2018
pos2019_filtered['YEAR'] = 2019

# Now, we can append the datasets.
pos_16to19_combined = pd.concat([pos2016_filtered, pos2017_filtered, pos2018_filtered, pos2019_filtered])
```

Now we can plot the number of observations in the combined dataset by year.

```{python}
# Grouping by year
pos_group_byyear = pos_16to19_combined.groupby('YEAR')
num_obs_peryear = pos_group_byyear.apply(lambda group: len(group))
num_obs_peryear = num_obs_peryear.reset_index()
num_obs_peryear.columns = ['YEAR', 'NUM_OBSERVATIONS']

# Creating the plot now
graph_num_obs_peryear = alt.Chart(num_obs_peryear).mark_line().encode(
    alt.X('YEAR:N', title = 'Year'),
    alt.Y('NUM_OBSERVATIONS:Q',  title = 'Number of observations',
    scale = alt.Scale(domain = [7200, 7400]))
).properties(
    title = 'Number of short-term hospitals over 2016-2019',
    width = 500,
    height = 200
)
(graph_num_obs_peryear + graph_num_obs_peryear.mark_point(fill = 'red')).display()
```

4. 
    a. Now, we will plot the number of unique hospitals the combined dataset per year

```{python}
# We can use the same group we built previously (by year)
num_unique_hospitals = pos_group_byyear.apply(lambda group: len(group['PRVDR_NUM'].unique()))
num_unique_hospitals = num_unique_hospitals.reset_index()
num_unique_hospitals.columns = ['YEAR', 'NUM_UNIQUE_HOSPITAL']

# Creating the new plot
graph_unique_hospital_peryear = alt.Chart(num_unique_hospitals).mark_line(color = 'red').encode(
    alt.X('YEAR:N', title = 'Year'),
    alt.Y('NUM_UNIQUE_HOSPITAL:Q',  title = 'Number of CMS certificaion number',
    scale = alt.Scale(domain = [7200, 7400]))
).properties(
    title = 'Number of unique hospitals over 2016-2019',
    width = 500,
    height = 200
)
(graph_unique_hospital_peryear + graph_unique_hospital_peryear.mark_point(fill = 'black')).display()
```
    b. This plot is exactly the same as the plot before. This tells two things that there might be facilities with the same name but with different CMS certification number. 

```{python}
# As a matter of fact, in 2016, we can see the 5 facilities with the highest number of different CMS certification number
print(pos2016_filtered.groupby('FAC_NAME').apply(lambda group: len(group['PRVDR_NUM'].unique())).sort_values(ascending = False).head(5))
```

This also tells that some of those facilities could be operating in multiple locations (meaning having different ZIP CODE in the dataset) or could be changing status or ownership from year to year.

## Identify hospital closures in POS file (15 pts) (*)

1. To find all the hospitals that were active in 2016 that were suspected to have closed by 2019, we will filter the 2016 data to find only the hospitals that are active providers first, then find contruct a function that can help us find if those hospitals are active or not by 2019. 
We came with this method while taking into account two other facts:

1- We will check if an hospital is still active for every year. The first year the hospital is not active, we will consider this year as the closure year. We won't consider the fact that hospitals that could have regain active status by 2019 and will assume it will get a new CMS certification code. Not the same one they had before.

2- If a CMS certification code was in the 2016 and not in any of the year, we will consider it as inactive. (More nuances will be addressed later). As we found earlier, the CMS certification code are also unique.

```{python}
# Filtering to find only active hospitals in 2016
pos2016_active = pos2016_filtered[pos2016_filtered['PGM_TRMNTN_CD'] == '0'].copy()
pos2016_active = pos2016_active.reset_index(drop = True)

# Building a function to determine if a hospital is active by 2019
def is_hospital_active(cms_instring, dataset):
    hospital_status = ''
    if cms_instring not in list(dataset['PRVDR_NUM']):
        hospital_status = 'Closure'
    elif cms_instring in list(dataset['PRVDR_NUM']):
        condition = [code == cms_instring for code in dataset['PRVDR_NUM']]
        if list(dataset[condition]['PGM_TRMNTN_CD'])[0] != '0':
            hospital_status = 'Closure'
        elif list(dataset[condition]['PGM_TRMNTN_CD'])[0] == '0':
            hospital_status = 'Active'
    return hospital_status

list_dataset  = [pos2017_filtered, pos2018_filtered, pos2019_filtered] 
STATUS = [] # Iniatilizing a column for the status of the hospital by 2019
YEAR_CLOSURE = [] # Iniatilizing a column for the year of closure if occured

# Checking for each Active hospital if they faced closure
for cms in pos2016_active['PRVDR_NUM']:
    year = 0
    while year < 3:
        if is_hospital_active(cms, list_dataset[year]) == 'Closure':
            STATUS.append('Closure')
            YEAR_CLOSURE.append(2017+year)
            break
        elif is_hospital_active(cms, list_dataset[year]) == 'Active':
            year += 1
    if year == 3:
        STATUS.append('Active')
        YEAR_CLOSURE.append(nan)
    else:
        pass
pos2016_active['STATUS'] = STATUS
pos2016_active['YEAR_CLOSURE'] = YEAR_CLOSURE

# We can then filter and select the columns only for the hospitals which become terminated or disappear from the data.
pos_closure_by2019 = pos2016_active[pos2016_active['STATUS'] == 'Closure']
pos_closure_by2019 = pos_closure_by2019.reset_index(drop = True)
pos_closure_by2019['YEAR_CLOSURE'] = [int(year) for year in pos_closure_by2019['YEAR_CLOSURE']] # To change the format of the data for the colum YEAR_CLOSURE (from float to int)
pos_closure_data = pos_closure_by2019[['FAC_NAME', 'ZIP_CD', 'STATUS', 'YEAR_CLOSURE']].copy()

print(f'The number of hospitals which become terminated or disappear from the data is {len(pos_closure_data)}')
```


2. Here is the list of first ten hospitals (ordered alphabetically) by name and year of suspected closure.

```{python}
sorted_closures = pos_closure_data.sort_values(by = 'FAC_NAME').reset_index(drop = True)
print(sorted_closures[['FAC_NAME','YEAR_CLOSURE']].head(10))
```

3. The first step to address the issue of false closure is to filter the last dataset we created "pos_closure_data" to only have the suspected hospital closure that occured before 2019.

```{python}
pos_closure_before2019 = pos_closure_data[pos_closure_data['YEAR_CLOSURE'] != 2019].copy()
pos_closure_before2019 = pos_closure_before2019.reset_index(drop = True)
```

Later, we will find any suspected hospital closures that are in zip codes where the number of active hospitals does not decrease in the year after the suspected closure. Then, we will remove those hospitals.

```{python}
# Filtering to find all active hospitals in each subsequent years after 2016
pos2017_active = pos2017_filtered[pos2017_filtered['PGM_TRMNTN_CD'] == '0'].copy()
pos2017_active = pos2017_active.reset_index(drop = True)

pos2018_active = pos2018_filtered[pos2018_filtered['PGM_TRMNTN_CD'] == '0'].copy()
pos2018_active = pos2018_active.reset_index(drop = True)

pos2019_active = pos2019_filtered[pos2019_filtered['PGM_TRMNTN_CD'] == '0'].copy()
pos2019_active = pos2019_active.reset_index(drop = True)

# Defining a function that allows us to find the number of active of active hospital per zip code in any specific year. We built this function as we will use this process several time in the next steps.

def num_activehospital_perzip(zip_instring, year):
    if year == 2016:
        number_hospital = sum(pos2016_active['ZIP_CD'] == zip_instring)
    elif year == 2017:
        number_hospital = sum(pos2017_active['ZIP_CD'] == zip_instring)
    elif year == 2018:
        number_hospital = sum(pos2018_active['ZIP_CD'] == zip_instring)
    elif year == 2019:
        number_hospital = sum(pos2019_active['ZIP_CD'] == zip_instring)
    else:
        print('Please enter a year between 2016 and 2019. This function also accept integers as input.')
    return number_hospital

TRUE_CLOSURE = [] # Initializing a variable to store the true status of an hospitals. This variable will contain booleans.

for zipcode in pos_closure_before2019['ZIP_CD']:
    year_now = int(pos_closure_before2019['YEAR_CLOSURE'][pos_closure_before2019['ZIP_CD'] == zipcode].iloc[0])
    year_after = year_now + 1
    if num_activehospital_perzip(zipcode, year_now) == 0:
        TRUE_CLOSURE.append(True) # We consider this as a true closure because there is literally no hospital in this zip_code for the year of closure. It's like this zip_code disappear in the dataset for the year of closure. This implies that it was not a merger or a change of name.
    elif num_activehospital_perzip(zipcode, year_after) < num_activehospital_perzip(zipcode, year_now):
        TRUE_CLOSURE.append(True) # A true closure because the number of hospital in the year after the suspected closure has decreased
    elif num_activehospital_perzip(zipcode, year_after) >= num_activehospital_perzip(zipcode, year_now):
        TRUE_CLOSURE.append(False) # A false closure (based on the method we used)

pos_closure_before2019['TRUE_CLOSURE'] = TRUE_CLOSURE
```

    a.
```{python}
print(f"Based on our calculation, {len(pos_closure_before2019) - sum(pos_closure_before2019['TRUE_CLOSURE'])} hospitals fit the definition of potentially being a merger/acquisition.")
```

    b. Let's remove the false closure from the dataset "pos_closure_data". We go back to include the closure in 2019 too. We assume here that all closures which happened in 2019 can be considered as true closures (this is a first pass method too.)

```{python}
# Finding the zip code for which there was a false closure
zip_false_closure = list(pos_closure_before2019[pos_closure_before2019['TRUE_CLOSURE'] == False]['ZIP_CD'])

# Creating a new column TRUE_STATUS in "pos_closure_data" as we might need it later.
TRUE_STATUS = [] # Initialing this column
for index in range(len(pos_closure_data)):
    # Unique index because there less unique zip code than number of rows.
    zip = pos_closure_data['ZIP_CD'][index]
    year = pos_closure_data['YEAR_CLOSURE'][index]
    if year == 2019:
        TRUE_STATUS.append('Closure') # As we assumed that all closure in 2019 are true closure
    elif year != 2019:
        if zip in zip_false_closure:
            TRUE_STATUS.append('Merger/Acquisition')
        elif zip not in zip_false_closure:
            TRUE_STATUS.append('Closure')

pos_closure_data['TRUE_STATUS'] = TRUE_STATUS

# Let's remove then all hospitals with their true status as merger/acquisition
pos_true_closure = pos_closure_data[pos_closure_data['TRUE_STATUS'] == 'Closure'].copy()
pos_true_closure = pos_true_closure.reset_index(drop = True)

print(f'Now, we are left with {len(pos_true_closure)} hospitals with true closure status')
```

    c. Here is the list of first ten hospitals (ordered alphabetically) by name for wchich their was a true closure.

```{python}
print(pos_true_closure.sort_values('FAC_NAME').head(10)[['FAC_NAME', 'ZIP_CD', 'TRUE_STATUS']]) 
``` 

## Download Census zip code shapefile (10 pt) 

1. 
    a. The 5 five file types are :
        - xml file (.xml), which stores text information
        - SHP file (.shp) has geometrics feature, geometric data
        - SHX file (.shx) has positional/shape index, which makes the access faster
        - DBF file (.dbf) has attribute information, with geometries
        - PRJ file (.prj) has the Coordinate Reference System (CRS)
    b. This is the size yypes are :
        - xml file (.xml) : 16 KB
        - SHP file (.shp) : 817915 KB
        - SHX file (.shx) : 259 KB
        - DBF file (.dbf) : 6275 KB
        - PRJ file (.prj) : 1 KB

2. 

```{python} 
shape_file_path = "N:/3 MES DOSSIERS SECONDAIRES/MASTER PREPARATION PROGRAM/University of Chicago/DAP II/problem-set-4-lara-ralph/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
zip_code_data = gpd.read_file(shape_file_path)
```

For Texas zip code, we will consider the 5-digit zip codes for which the first 3 digits ranges from 750 to 799 and the 5-digit zip code which starts with 733, which is associated to Austin, TX too.
# Reference
# https://simple.wikipedia.org/wiki/List_of_ZIP_Code_prefixes 

```{python}
# Creating the 3 first digits for Texas zip code
first_3digits_txzip = ['733'] # Inializing the list of zip code for Texas
[first_3digits_txzip.append(str(num)) for num in range(750, 800)]

# Filtering the "zip_code_data" to find only the zip code in Texas
condition_zip_tx = [zip_code_data['ZCTA5'][index][:3] in first_3digits_txzip for index in range(len(zip_code_data))]
zip_code_texas = zip_code_data[condition_zip_tx].copy()
zip_code_texas = zip_code_texas.reset_index(drop = True)
```

Let's find now the number of hospitals per zip code in 2016 based on the cleaned POS datasets (pos_true_closure) we created in the previous section. 

```{python}
# Filtering the datasets for active hospitals in 2016, to only consider the zip code for which that has been a true closure for an hospital (in pos_true_closure)
condition_closure_pos = [zipcode in list(pos_true_closure['ZIP_CD']) for zipcode in list(pos2016_active['ZIP_CD'])]

pos2016_zip_closure = pos2016_active[condition_closure_pos].copy()
pos2016_zip_closure = pos2016_zip_closure.reset_index(drop = True)

# We can then find the number of hospitals per zip code in 2016 using the new filtered dataset
group_zip2016_closure = pos2016_zip_closure.groupby('ZIP_CD')
num_hospital_perzip2016_closure = group_zip2016_closure.apply(lambda group: len(group))
num_hospital_perzip2016_closure = num_hospital_perzip2016_closure.reset_index()
num_hospital_perzip2016_closure.columns = ['ZIP_CD', 'NUMBER_HOSPITAL_1']
```

Finally, for the choropleth, we will use two approaches as the instructions were ambiguous on which plot we should create:

1- We will create a choropleth of the number of hospitals by zip code in Texas, based on the true closure data. We are still considering the year 2016. Since the "zip_code_texas" doesn't have the number of hospitals per zip code, we will create a column. However, for any zip code that are in zip_code_texas and not present in the POS 2016 datasets for zip codes where there has been closures (pos2016_zip_closure), we will consider the number of hospital in 2016 as 0.

2- The second approach is more straight forward, we will NOT just consider zip code where there has been closures, but all zip code for Texas. This is a precaution in case this is what the instructions was asking for.


 A- FIRST APPROACH

```{python}
NUMBER_HOSPITAL_1 = [] # Initializing the new columns that will be inserted in zip_code_texas

# Finding the number of hospital for each Texas zip code
for zipcode in list(zip_code_texas['ZCTA5']):
    if zipcode not in list(num_hospital_perzip2016_closure['ZIP_CD']):
        NUMBER_HOSPITAL_1.append(0)
    elif zipcode in list(num_hospital_perzip2016_closure['ZIP_CD']):
        num_for_thiszip = int(num_hospital_perzip2016_closure['NUMBER_HOSPITAL_1'][num_hospital_perzip2016_closure['ZIP_CD'] == zipcode].iloc[0])
        NUMBER_HOSPITAL_1.append(num_for_thiszip)

zip_code_texas['NUMBER_HOSPITAL_1'] = NUMBER_HOSPITAL_1
```

Let's create the choropleth now. But first we found a way to create our own colormap using ChatGPT by giving it the following prompt :
# "How to create a custom colormap with only four distinct colors"

```{python}
print(f"There {len(zip_code_texas['NUMBER_HOSPITAL_1'].unique())} unique values for the number of hospitals, which are {zip_code_texas['NUMBER_HOSPITAL_1'].unique()}")

# That's why we are customizing 4 colors
color_num_hospital = ['#e8e892', '#eb3434', '#010b78', '#23ed11'] 
cmap_num_hospital = mcolors.ListedColormap(color_num_hospital)

# Customizing the ticks too
tick_num_hospital = [0, 1, 2, 4]
tick_num_hospital_labels = ['0', '1', '2', '4']

# Now finalizing the plot
graph_hospital_byzip_texas = zip_code_texas.plot(column = 'NUMBER_HOSPITAL_1', cmap = cmap_num_hospital)
plt.axis("off")
plt.title('Number of hospitals in 2016 per zip code in Texas,\n based on closure happening by 2019', fontsize = 13)
color_used = plt.colorbar(graph_hospital_byzip_texas.collections[0])
color_used.set_ticks(tick_num_hospital)  # Set the ticks you want
color_used.set_ticklabels(tick_num_hospital_labels)  # Set the labels for those ticks
color_used.set_label('Number of Hospitals', fontsize = '8')

graph_hospital_byzip_texas
```

 B- SECOND APPROACH (This is an alternative interpretation of the question)
We will use all zip code for Texas (not just the one in which there has been closure)

```{python}
# FindINg the number of hospitals per zip code in 2016 using the new filtered dataset
group_zip2016 = pos2016_active.groupby('ZIP_CD')
num_hospitaltx_perzip2016 = group_zip2016.apply(lambda group: len(group))
num_hospitaltx_perzip2016 = num_hospitaltx_perzip2016.reset_index()
num_hospitaltx_perzip2016.columns = ['ZIP_CD', 'NUM_HOSPITAL_2']

# Initializing the new columns that will be inserted in zip_code_texas
NUMBER_HOSPITAL_2 = [] 

# Finding the number of hospital for each Texas zip code
for zipcode in list(zip_code_texas['ZCTA5']):
    if zipcode not in list(num_hospitaltx_perzip2016['ZIP_CD']):
        NUMBER_HOSPITAL_2.append(0)
    elif zipcode in list(num_hospitaltx_perzip2016['ZIP_CD']):
        num_for_thiszip = int(num_hospitaltx_perzip2016['NUM_HOSPITAL_2'][num_hospitaltx_perzip2016['ZIP_CD'] == zipcode].iloc[0])
        NUMBER_HOSPITAL_2.append(num_for_thiszip)

zip_code_texas['NUMBER_HOSPITAL_2'] = NUMBER_HOSPITAL_2

# Going to the plot now

print(f"There {len(zip_code_texas['NUMBER_HOSPITAL_2'].unique())} unique values for the number of hospitals, which are {zip_code_texas['NUMBER_HOSPITAL_2'].unique()}")

# That's why we are customizing 6 colors
color_num2_hospital = ['#e8e892','#eb3434','#9608a3', '#010a02', '#010b78', '#23ed11'] 
cmap_num2_hospital = mcolors.ListedColormap(color_num2_hospital)

# Customizing the ticks too
tick_num2_hospital = [0, 1, 2, 3, 4, 5]
tick_num2_hospital_labels = ['0', '1', '2', '3', '4', '5']

# Now finalizing the plot
graph2_hospital_byzip_texas = zip_code_texas.plot(column = 'NUMBER_HOSPITAL_2', cmap = cmap_num2_hospital)
plt.axis("off")
plt.title('Number of hospitals in 2016 per zip code in Texas', fontsize = 13)
color_used2 = plt.colorbar(graph2_hospital_byzip_texas.collections[0])
color_used2.set_ticks(tick_num2_hospital)  # Set the ticks you want
color_used2.set_ticklabels(tick_num2_hospital_labels)  # Set the labels for those ticks
color_used2.set_label('Number of Hospitals', fontsize = '8')

graph2_hospital_byzip_texas
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1.

```{python}
# Calculating centroid for each zip code
zips_all_centroids = zip_code_data.copy()
zips_all_centroids['centroid'] = zips_all_centroids.geometry.centroid

# Showing the dimensions of the new dataframe
dimensions = zips_all_centroids.shape 
dimensions
```
The dimensions reflect the number of zip codes nationwide, with columns similar to those in the original zip_code_data GeoDrataFrame but with updated geometries to show centroids rather than full shapes.

```{python}
print(f'The columns of the new geodataframe are:\n {list(zips_all_centroids.columns)}')
```

GEO_ID: is a unique identifier for each geographic entity, often assigned by the census or dataset provider.
ZCTA5: the 5-digit ZIP Code Tabulation Area, representing each zip code area.
NAME: the name of the geographic area, matching the zip code too.
LSAD: the Legal/Statistical Area Description, which categorizes geographic areas (e.g., "ZCTA" for Zip Code Tabulation Area).
CENSUSAREA: the area of the ZIP Code Tabulation Area in square units, as defined in the dataset.
geometry: The spatial representation (including the boundaries)
centroid : The centroid point of each zip code polygon, represented as a Point geometry.

2. 
```{python}
zips_texas_centroids = zip_code_texas.copy()
zips_texas_centroids['centroid'] = zips_texas_centroids.geometry.centroid
zips_texas_centroids = zips_texas_centroids[['GEO_ID', 'ZCTA5', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry', 'centroid', 'NUMBER_HOSPITAL_1', 'NUMBER_HOSPITAL_2']]
```

We researched Texas border states and found four bordering states as results: Oklahoma (OK), Arkansas (AR), Louisiana (LA), and New Mexico (NM).
# https://www.worldatlas.com/articles/which-states-border-texas.html

```{python}
# Let's define ZIP codes prefixes for Texas' boredering states (used chatgpt to identify the prefixes)
louisiana_prefixes = list(map(str, range(700, 715))) # LA
arkansas_prefixes = list(map(str, range(716, 730))) # AR
oklahoma_prefixes1 = list(map(str, range(730, 733))) # OK
oklahoma_prefixes2 = list(map(str, range(734, 750)))
oklahoma_prefixes = oklahoma_prefixes1 + oklahoma_prefixes2
new_mexico_prefixes = list(map(str, range(870, 885))) # NM

border_states_prefixes = new_mexico_prefixes + oklahoma_prefixes + arkansas_prefixes + louisiana_prefixes

# Creating the GeoDataFrame for Texas and Bordering States by filtering zip_Code_data
zips_texas_borderstates_centroids = zip_code_data[zip_code_data['ZCTA5'].str[:3].isin(border_states_prefixes + first_3digits_txzip)].copy()
zips_texas_borderstates_centroids['centroid'] = zips_texas_borderstates_centroids.geometry.centroid

# Now checking for unique numbers of ZIP codes in each subset
unique_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
unique_border_states_zips = zips_texas_borderstates_centroids['ZCTA5'].nunique()

print(f'There are {unique_texas_zips} uniques zip codes for Texas.')
print(f'There are {unique_border_states_zips} uniques zip codes for Texas and its bordering states')
```

3. 

```{python}
# Creating a dataset of all active hospitals per ZIP code in 2016
num_hospital_perzip2016 = pos2016_active.groupby('ZIP_CD').size().reset_index(name = 'NUM_HOSPITAL')

# Renaming the zip code column so it can match the name of the zip code colum in "zips_texas_borderstates_centroids"
num_hospital_perzip2016.rename(columns = {'ZIP_CD': 'ZCTA5'}, inplace = True)

# Merging both datasets on ZCTA5
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    num_hospital_perzip2016,
    on = "ZCTA5",
    how = "inner"
)

# Filtering for ZIP codes with at least 1 hospital (NUM_HOSPITAL >= 1)
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids['NUM_HOSPITAL'] >= 1]

# Dropping NUMBER_HOSPITAL to avoid redundancy since NUM_HOSPITAL includes our active hospitals)
if 'NUMBER_HOSPITAL' in zips_withhospital_centroids.columns:
    zips_withhospital_centroids.drop(columns = 'NUMBER_HOSPITAL', inplace = True)
else:
    pass

print(f'The number of ZIP codes in Texas and its bordering states, with at least one hospital is {len(zips_withhospital_centroids)} ZIP codes.')
```

We used inner join to merge zip_texas_borderstates_centroids with num_hospital_perzip2016. We performed the merge on the ZCAT5 column and excluded the NUMBER_HOSPITAL column which has been generated from border criteria that included inactive hospitals, data from diff years, etc and kept instead NUM_HOSPITAL since it was calculated from a dataset that is already filtered to include only hospitals that were active in 2016.

4.  To find the distance of a Texas zip code to the nearest zip code with at least one hospital in zips_withhospital_centroids, we will a spatial join to compare each zip code in Texas to all the zip codes in zips_withhospital_centroids. The method sjoin_nearest automatically exclude comparing the zip code to itself in the right geodataframe. This approach will allow us to consider not just zip code outside of Texas, but also zip code in Texas

    a.
```{python}
# Subsetting to test this on 10 zip code in Texas
subset_zips_texas = zips_texas_centroids.tail(10) 
start_time = time.time() # Recording starting time

# Performing the spatial join
subset_nearest_zipcode = subset_zips_texas.sjoin_nearest(
    zips_withhospital_centroids[['ZCTA5', 'geometry']],  
    how = "left",  
    distance_col = "min_distance_to_hospital"  
)
elapsed_time = time.time() - start_time # Recording end time

print('Nearest zip code for the subset of 10 ZIP codes\nand the distance of those zip code in Texas to their nearest zip code:\n', subset_nearest_zipcode[['ZCTA5_left', 'ZCTA5_right', 'min_distance_to_hospital']], '\n')
print(f"Time taken for to find the nearest zip code with at least one hospital for a subset 10 ZIP codes in Texas: {round(elapsed_time, 2)} seconds\n")

# Finding the estimated time for the entire procedure
estimated_elapsed_time_full = (len(zips_texas_centroids) * elapsed_time) / 10
print(f'The estimated time for the entire procedure would be: {round(estimated_elapsed_time_full / 60, 2)} minutes')
```

    b. We'll follow the same approach as the above section but now we're doing the calculation for the entire dataset "zips_texas_centroids".

```{python}
start_time = time.time()
full_nearest_zipcode = zips_texas_centroids.sjoin_nearest(
    zips_withhospital_centroids[['ZCTA5', 'geometry']],
    how = "left", 
    distance_col = "min_distance_to_hospital"  
)
elapsed_time_full = time.time() - start_time

print('Nearest zip code for the full dataset and the distance of each Texas zip code to its nearest zip code:\n', full_nearest_zipcode[['ZCTA5_left', 'ZCTA5_right', 'min_distance_to_hospital']].head())
print(f"Time taken for the full dataset: {round(elapsed_time_full / 60, 2)} minutes")

# Finding how close the time to run the full procedure is to our estimation

print(f'The difference between the effective time to run the full procedure and the estimated time based on the how long it took to run the procedure on a subset of 10 zip codes is {elapsed_time_full - estimated_elapsed_time_full} seconds.')
```

    c.
The file indicates that the degree is in degrees of latitude.

Base on our research, conversion factor is : 1 degree of latitude ≈ 69 miles
# https://www.thoughtco.com/degree-of-latitude-and-longitude-distance-4070616

5. We calculated earlier in the previous question the average distance to the nearest zip code for each zip code in Texas. This distance is a proxy of a the average distance to the nearest hospital.

```{python}
print(full_nearest_zipcode[['ZCTA5_left', 'min_distance_to_hospital']])
```
    a. The unit is in degrees as we stated before.

    b. Since we already had the conversion factor (1 degree ≈ 69 milles), we can calculate the average distance to the nearest hospital in miles.

```{python}
# Setting the conversion factor
conversion_factor = 69  

# Converting the distance to the nearest hospital in miles
full_nearest_zipcode['min_distance_to_hospital_miles'] = full_nearest_zipcode['min_distance_to_hospital'] * conversion_factor
```
Some of those values don't make sens as some of them seems to indicate 0 miles as the average distance. However, one possible explanation we can give is that the centroid of the two zip code compared in those cases are so close (potentially because of the geometric configuration of the zip code), that the distance in terms of degree is almost zero. This could explain why we have those values.

    c.

```{python}
graph_min_distance_byziptx = full_nearest_zipcode.plot(column = 'min_distance_to_hospital_miles', cmap = 'plasma', legend = True)
plt.axis("off")
plt.title('Texas zip codes based on the average distance\nto the nearest hospital in 2016', fontsize = '15', loc = 'center')
legends = plt.legend(title ='Average distance to nearest hospital (miles)', title_fontsize = '10', fontsize = '10', loc = 'right', bbox_to_anchor = (1.5, 0.5))
legends.get_title().set_rotation(90)
plt.gcf().set_facecolor('lightgray')

graph_min_distance_byziptx
```
    
## Effects of closures on access in Texas (15 pts)

1. Using the corrected hospital closures dataset from the first section, we will create
a list of directly affected zip codes in Texas (those with at least one closure in
2016-2019.)

```{python}
# Filtering for texas zip codes
texas_zip_with_closures = pos_true_closure[pos_true_closure['ZIP_CD'].str[:3].isin(first_3digits_txzip)]

# Group by ZIP code and count 
texas_closures_by_zip = texas_zip_with_closures.groupby('ZIP_CD').size().reset_index(name = 'num_closures')

print("The number of closures by Texas ZIP code is:\n", texas_closures_by_zip)
```

2. Then, we are plotting a choropleth of which Texas zip codes were directly affected by a closure in 2016-2019

```{python}
# Merging texas_closures_by_zip with the Texas ZIP code shapefile to collect unaffected Texas ZIP codes and the spatial data required for the map
zip_code_texas_numclosure = zip_code_texas.copy().merge(
    texas_closures_by_zip, 
    left_on = 'ZCTA5', 
    right_on = 'ZIP_CD', 
    how = 'left', 
    suffixes = ('', '_closures')
)

# All the NaN values represent zip code cases where there was not a closure. We will convert them to "0".
zip_code_texas_numclosure ['num_closures'] = zip_code_texas_numclosure ['num_closures'].fillna(0).astype(int)

# Creating a column to indicate if there was at least one closure and which  zip code where affected by closures.
zip_code_texas_numclosure['affected_byclosure'] = zip_code_texas_numclosure['num_closures'].apply(lambda num_closure: 1 if num_closure > 0 else 0)

# Creating the choropleth map now. 
# fig, ax = plt.subplots(1, 1, figsize = (10, 10))
#### We dont' really need this line as the axis is not important here
# Since theree are only two value for the variable "affected_byclosure", we will first customize a color palette with only 2 colors and two ticks.

color_affected_closure = ['#04098f', '#eded0c'] 
cmap_affected_closure = mcolors.ListedColormap(color_affected_closure)
tick_affected_closure = [0, 1]
tick_affected_closure_labels = ['Not Affected', 'Affected']

graph_txaffected_byclosure = zip_code_texas_numclosure.plot(
    column = 'affected_byclosure', 
    cmap = cmap_affected_closure, 
    linewidth = 0.8
)
plt.title("Situation of Texas by Zip Code regarding\nthe hospital closures between 2016 and 2019", fontsize = '15', loc = 'center')
color_used_closure = plt.colorbar(graph_txaffected_byclosure.collections[0])
color_used_closure.set_ticks(tick_affected_closure)  # Setting the ticks we want
color_used_closure.set_ticklabels(tick_affected_closure_labels)  # Setting the labels for ticks
legends = plt.legend(title ='Situation by zip code regarding closure:\n        1=Affected ; 0=Not Affected', title_fontsize = '11', fontsize = '10', loc = 'right', bbox_to_anchor = (1.5, 0.5))
legends.get_title().set_rotation(90)
plt.axis("off")

graph_txaffected_byclosure

# Now counting the number of directly affected ZIP codes
num_directly_affected = zip_code_texas_numclosure['affected_byclosure'].sum()
print(f"The number of ZIP codes in Texas directly affected by hospitals closures is: {num_directly_affected} hospitals.")
```

3. Next, we will identify all the indirectly affected zip codes, which means Texas zip codes within a 10-mile radius of the directly affected zip codes.

```{python}
zip_code_texas_situation = zip_code_texas_numclosure[['ZCTA5', 'affected_byclosure', 'geometry']].copy()

# Filtering the zip_code_texas_situation to include only ZIP codes that were directly affected
zip_codetx_directly_affected = zip_code_texas_situation[zip_code_texas_situation['affected_byclosure'] == 1]

# Since EPSG:4269 is a geographic CRS (degrees), we approximate 10 miles as 0.145 degrees
buffer_distance = 0.145

# Let's create a 10-mile buffer around directly affected ZIP codes
zip_codetx_directly_affected_buffer = zip_codetx_directly_affected.copy()
zip_codetx_directly_affected['geometry'] = zip_codetx_directly_affected_buffer.geometry.buffer(buffer_distance)

# Performing a spatial join to find indirectly affected ZIP codes
zip_code_indirectly_affected = gpd.sjoin(
    zip_code_texas_situation, 
    zip_codetx_directly_affected_buffer[['geometry']],  
    how = "inner", 
    predicate = "intersects",
    lsuffix = '_texas', 
    rsuffix = '_buffer'
)

# Filtering out indirectly affected ZIP codes from the result
zip_code_indirectly_affected = zip_code_indirectly_affected[zip_code_indirectly_affected['affected_byclosure'] == 0]

# Count the number of indirectly affected ZIP codes
num_indirectly_affected = zip_code_indirectly_affected['ZCTA5'].nunique()
print(f"The number of indirectly affected ZIP codes by closures in Texas is: {num_indirectly_affected}")
```

4. Finally, let's Make a choropleth plot of the Texas zip codes with a different color for each of the 3 categories.

```{python}
zip_code_texas_final = zip_code_texas[['ZCTA5', 'geometry']].copy()

# Merge `zip_code_texas_final` with `zip_code_texas_numclosure` to add `affected_byclosure` column
zip_code_texas_final = zip_code_texas_final.merge(
    zip_code_texas_numclosure[['ZCTA5', 'affected_byclosure']], 
    on = 'ZCTA5', 
    how = 'left'
)
```

To define the different categories, we will go through three steps:

--> STEP 1: Merginig the final dataset with the dataset of indirectly affected zip code, while creating the indicator column to mark indirectly affected ZIP codes.
The "left_only" value will tell us the unaffected at all and affected directly zip codes.
The "both" will tell us the indirectly affected zip code

```{python}
zip_code_texas_final = zip_code_texas_final.merge(
    zip_code_indirectly_affected[['ZCTA5']], 
    on = 'ZCTA5', 
    how = 'left', 
    indicator = 'indirectly_affected'
)
```

--> STEP 2: Definin an new column name 'impact_category' column and set the value of this column to "Directly Affected" where "affected_byclosure == 1" and "Unaffected" where "affected_byclosure == 0" (we will later differentiate between the two types of Unaffected : which are unaffected at all and indirectly affected).

```{python}
zip_code_texas_final['impact_category'] = zip_code_texas_final['affected_byclosure'].apply(
    lambda affected: 'Directly Affected' if affected == 1 else 'Unaffected'
)
```

--> STEP 3: From the category "Unaffected, we will differentiate the zip codes thare are unaffected at all and those that are indirectly affected. We will label the last ones as "Indirectly Affected".

```{python}
zip_code_texas_final.loc[
    (zip_code_texas_final['indirectly_affected'] == 'both') & (zip_code_texas_final['impact_category'] == 'Unaffected'), 
    'impact_category'
] = 'Indirectly Affected'
```

Finally, we can make the choropleth map

```{python}
fig, graph_category_closure = plt.subplots(1, 1, figsize = (20, 10))
zip_code_texas_final.plot(
    column = 'impact_category', 
    cmap = 'Set1', 
    legend = True, 
    linewidth = 0.8, 
    ax = graph_category_closure, 
    edgecolor = '0.8'
)
graph_category_closure.set_title('Situation of Texas ZIP Codes\nregarding hospitals closures  from 2016 to 2019', fontsize = '20', loc = 'center')
graph_category_closure.axis("off")
graph_category_closure
```

## Reflecting on the exercise (10 pts) 
1.
The "first-pass" method identifies closures by assuming that hospitals active in 2016 but absent in subsequent years have permanently closed. Although this captures a baseline of closures, several limitations remain:

a. New Openings Offset by Closures: The method doesn’t account for cases where a hospital closure in a ZIP code is offset by new hospital openings nearby. This means the true impact on healthcare access might be overstated if other facilities are accessible within or near the same ZIP code.

b. Limitations in Using 2019 Data: Since we lack data for 2020, we assumed all hospitals not present in 2019 data were permanently closed, which may overstate closures. A more precise method would require data beyond 2019 to confirm whether these facilities remained closed.

c. Mergers, Acquisitions, and Potential Relocations: While we addressed mergers and acquisitions later on, our method does not fully consider hospitals that may have relocated to a different ZIP code but did not close. This can lead to misclassifications if the hospital’s certification number changes following relocation.

d. Reliability of Multi-Year Checks: While the method applies multi-year checks, it focuses primarily on initial closures and does not capture facilities that may close temporarily and reopen within the same or subsequent years. This limitation is especially relevant for hospitals that may have had temporary interruptions, especially around 2018–2019.

Improvements:

i. Track Facility Name and Certification Numbers Across Years: We have access to facility names and certification numbers. By tracking each certification number that disappears, we could cross-check the facility name in the following year. This approach would allow us to verify if the facility reappears with a new certification number in the same or a nearby ZIP code, indicating a potential relocation rather than a closure.

ii. Cross-Reference with External Data Sources: To confirm true closures more effectively, cross-referencing with external records like state health department data, American Hospital Association records, or news reports could verify if a facility permanently ceased operations. This would reduce potential false positives from our internal dataset alone.

iii. Geospatial Analysis for Neighboring ZIP Codes: Applying geospatial analysis could help assess healthcare access changes across neighboring ZIP codes, particularly if the closed facility is near other hospitals. This analysis would provide a more nuanced view of actual access impacts by identifying regions where access remains viable despite nearby closures.

2.
The current approach identifies ZIP codes as “affected” by closures based on whether a hospital within the ZIP code has closed or if the ZIP code falls within a 10-mile radius of a directly affected ZIP code. While this method captures a general area impacted by closures, it has some limitations in reflecting true access changes:

a. Proximity Doesn’t Equal Accessibility: A 10-mile radius might not account for actual travel time, barriers (such as highways or geographic obstacles), or public transportation availability. Access is not merely about distance; it involves practical considerations in reaching a facility, particularly in rural or underserved areas. An improvement would be to use travel time analysis rather than a simple distance buffer to better capture realistic access for residents.

b. Overlooking ZIP Code Density and Population Needs: Not all ZIP codes have the same population density or healthcare demand. Our approach treats all affected ZIP codes similarly, which may underrepresent the impact on densely populated or high-need areas. Incorporating population density, age distribution, and health statistics would provide a more nuanced measure of how closures impact healthcare accessibility across different communities.

c. Ignoring Alternative Providers and Service Consolidation: Closures do not necessarily imply a complete loss of access, as nearby facilities may absorb demand or expand services. By identifying only ZIP codes with closures, we may miss cases where neighboring facilities are adequately compensating for the closure. Adding metrics on healthcare capacity in surrounding areas could reveal whether alternative providers are available, thereby improving the accuracy of the impact measure.

d. New Facilities Offsetting Closures: If a hospital closes in a ZIP code, but other healthcare facilities open nearby, the impact on healthcare access could be less severe than the closure data alone suggests. Our method doesn’t currently consider these offsetting factors, potentially leading to an overstatement of access reductions in affected ZIP codes.